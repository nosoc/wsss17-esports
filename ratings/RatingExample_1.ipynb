{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rating systems can help us in solving a number of problems:\n",
    "\n",
    "1. Ordering all players by skills. \n",
    "2. Predicting the winner. \n",
    "3. Forming teams of similar strength.\n",
    "\n",
    "We will try to discuss each problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different rating systems. We will consider three of them:\n",
    "1. Elo (https://en.wikipedia.org/wiki/Elo_rating_system)\n",
    "2. Glicko/Glicko 2 (http://www.glicko.net/glicko.html)\n",
    "3. TrueSkill (https://www.microsoft.com/en-us/research/project/trueskill-ranking-system/)\n",
    "\n",
    "For better understanding of TrueSkill model we higly recommend to read excelent description by Jeff Moser (http://www.moserware.com/2010/03/computing-your-skill.html) and related paper (http://www.moserware.com/assets/computing-your-skill/The%20Math%20Behind%20TrueSkill.pdf). This links also relevant to Elo ratings, since it considered as basic model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use dataset from http://dotascience.com/. You can obtain data by link http://dotascience.com/data/dotahack_online_data.zip. At this class we will use only information about matches results (selected_team_matches.csv), rather than all data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea of rating systems is to find parameters in specific math models that gives better prediction of winer, or better prediction of scores obtained in one game.\n",
    "\n",
    "For Elo rating we have a simple model that estimates expected score for player A from one game with player B as $$E_A = \\frac 1 {1 + 10^{(R_B - R_A)/400}},$$ where $R_A$ and $R_B$ are current ratings of player A and player B respectively.\n",
    "\n",
    "Obviously:\n",
    "* If $R_A = R_B$ it means that estimated score is equal 0.5.\n",
    "* If $R_A = R_B + 400$ it means that estimated score is equal 10/11, i.e. player A got ten times more score than player B. If the only outcomes are win or lose (no draws), player A wins ten times more frequently than player B i.e. the probability of wining is 10/11.\n",
    "\n",
    "***Question:*** Can you calculate rating difference that provides player A an estimate of 75% of the total score?\n",
    "\n",
    "You should get approxemately 190.85."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model above provides you with a chances of win or estimation of scores with given players ratings, but does not give you any clue how to estimate this ratings.\n",
    "\n",
    "The idea is very simple: if you score more than expected you get rating boost, if you score less than expected you lose some score points.\n",
    "\n",
    "The exact formula for changes in Elo rating of player A is $$R_{A}^{\\prime }=R_{A}+K(S_{A}-E_{A}),$$ where $S_A$ is real score of player A, $E_A$ - expected scores estimated by formula described above, an $K$ is a coefficient controlling the speed of rating changes. Usualy we use large $K$ for newcomers and smaller $K$ for players with a long history. You may check [wiki page](https://en.wikipedia.org/wiki/Elo_rating_system#Most_accurate_K-factor) for examples of different K schemas.\n",
    "\n",
    "_Note_: if you use the same $K$ for each player you just redistribute rating points among players.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing that we should discuss in connection with Elo rating is initialisation of rating for new players. \n",
    "\n",
    "Since the only factor that affect our predictions is the difference in ratings rather than their absolute value, we can arbitrary chouse any value for newbees. Usualy this is either an exact number e.g. 1000, or mean/median value of all players. In our model we will use 1000 points as a start rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Elo rating system is very simple, so we can reimpliment it for an educational purpose. We will need a function that recalculate ratings with respect to the match outcome. It also will be usefull to have a fuction estimating chance of win, and since there are no draws in our it will be very straitforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Elo_win_probability(player_rating, opponent_rating):\n",
    "    delta_rating = player_rating - opponent_rating\n",
    "    return 1/(1+10**(-delta_rating/400.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check if our function return expected results on few example cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Elo_win_probability(1000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7500016169640152"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Elo_win_probability(1190.85, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It provides us with expected results. Let's implement function that returns updated ratings. We will use the same $K=30$ for every game, but you may implement $K$ changes later.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Elo_update_rates(winer_rating, loser_rating, winer_K = 30, loser_K = 30):\n",
    "    winer_estimated_score = Elo_win_probability(winer_rating, loser_rating)\n",
    "    loser_estimated_score = 1 - winer_estimated_score # Estimated scores should sum up to 1    \n",
    "    winer_new_rating = winer_rating + winer_K *(1 - winer_estimated_score)\n",
    "    loser_new_rating = loser_rating + loser_K *(0 - loser_estimated_score)\n",
    "    return (winer_new_rating, loser_new_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1015.0, 985.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Elo_update_rates(1000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have function that recalculate rating, so we can calculate ratings for each team in our dataset. We will consider each game separately, but you shoud later think how to deal with \"best of 3\" or \"best of 5\" series as with one event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "matches_info = pd.read_csv('selected_team_matches.csv').sort_values(['match_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tournament</th>\n",
       "      <th>radiant</th>\n",
       "      <th>dire</th>\n",
       "      <th>winner</th>\n",
       "      <th>match_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-06-24</td>\n",
       "      <td>The Defense 2</td>\n",
       "      <td>WhA</td>\n",
       "      <td>EG</td>\n",
       "      <td>DIRE</td>\n",
       "      <td>22270148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-06-28</td>\n",
       "      <td>The Defense 2</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>EG</td>\n",
       "      <td>DIRE</td>\n",
       "      <td>22959375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-06-29</td>\n",
       "      <td>StarSeries II Finals</td>\n",
       "      <td>EG</td>\n",
       "      <td>Empire</td>\n",
       "      <td>DIRE</td>\n",
       "      <td>23152391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-06-29</td>\n",
       "      <td>StarSeries II Finals</td>\n",
       "      <td>EG</td>\n",
       "      <td>NEXT.kz</td>\n",
       "      <td>RADIANT</td>\n",
       "      <td>23160256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-07-11</td>\n",
       "      <td>The Defense 2</td>\n",
       "      <td>EG</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>DIRE</td>\n",
       "      <td>25449321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date            tournament  radiant     dire   winner  match_id\n",
       "0  2012-06-24         The Defense 2      WhA       EG     DIRE  22270148\n",
       "1  2012-06-28         The Defense 2  Unknown       EG     DIRE  22959375\n",
       "2  2012-06-29  StarSeries II Finals       EG   Empire     DIRE  23152391\n",
       "3  2012-06-29  StarSeries II Finals       EG  NEXT.kz  RADIANT  23160256\n",
       "4  2012-07-11         The Defense 2       EG  Unknown     DIRE  25449321"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a dictionary with current rates. After that we iterate through all matches and recalculate ratings acording to each match outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Elo_ratings = {}\n",
    "start_rating = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index, row in matches_info.iterrows():\n",
    "    team_radiant = row['radiant']\n",
    "    team_dire = row['dire']\n",
    "    if team_radiant not in Elo_ratings:\n",
    "        Elo_ratings[team_radiant] = start_rating\n",
    "    if team_dire not in Elo_ratings:\n",
    "        Elo_ratings[team_dire] = start_rating\n",
    "    if  row['winner'] == \"RADIANT\":\n",
    "        Elo_ratings[team_radiant], Elo_ratings[team_dire] = Elo_update_rates(Elo_ratings[team_radiant], Elo_ratings[team_dire])\n",
    "    else:\n",
    "        Elo_ratings[team_dire], Elo_ratings[team_radiant] = Elo_update_rates(Elo_ratings[team_dire], Elo_ratings[team_radiant])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have Elo ratings of every team. Lets find the leaders and losers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "team_sorted_by_Elo = sorted(Elo_ratings, key=Elo_ratings.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EHOME', 'Alliance', 'OG', 'VP 2', 'EG']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team_sorted_by_Elo[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1391.782235380362,\n",
       " 1347.6644664248986,\n",
       " 1311.7109234075542,\n",
       " 1277.8484821634563,\n",
       " 1276.8960645309862]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Elo_ratings[team] for team in team_sorted_by_Elo[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SiG.Tr', 'LvT', 'EHUG', 'DK', '4ASC']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team_sorted_by_Elo[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[912.6701186183126,\n",
       " 906.1960947912388,\n",
       " 896.9281893130452,\n",
       " 894.738858505895,\n",
       " 888.0133892079333]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Elo_ratings[team] for team in team_sorted_by_Elo[-5:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can estimate winning chances of Virtus pro (with a tag 'VP 2') in the match against Natus Vincere (with a tag 'NaVi')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7911955410911068"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Elo_win_probability(Elo_ratings['VP 2'], Elo_ratings['NaVi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know the basics of rating systems. \n",
    "\n",
    "Elo rating system is good and quite useful but it has some issues. The main issue of Elo system is the lack of information about player's rating uncertainty. Consider the person with rating 1000. What does it mean? If the person played 1000 games it means that it is an average player, but if the person played only 2 games it means almost nothing. \n",
    "How can we deal with such uncertanty?\n",
    "And can we deal with teams of players that can change from game to game?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answers for questions above is TrueSkill. The TrueSkill is the system that allows you to compare \"teams\" that are not stable when it comes to players. For example if I play any sesional game that assigns me to a random team in each match how the result of game should affect on estimation of my perfomance? Another possible case is if we have teams with defined members, but often one or two of this members can not play and team plays with a guest member, usualy caled \"legioner\". Shoud we use information of such game to evaluate team rating or not?\n",
    "\n",
    "TrueSkill considers following basic ideas:\n",
    "1. Each player has his own rating defined by mean and variance.\n",
    "2. In each game we assume that player perfomance in the game is a random value normally distributed with mean and variance from the player's rating.\n",
    "3. Team perfomance in each game is a sum of performance of players in this game. \n",
    "\n",
    "All above lead to the notion of team perfomance in every game as some normaly distributed random value, so we can estimate winning probabilities. And since the model can be described in terms of random variables distributions we can use Bayes theorem to update player ratings with respect to outcome. Since the team rating is just sum of player ratings, we can ignore teams: if we nead team rating we just calculate it based on member list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all lets use TrueSkill for estimates rating for the same data that we use previously. We will use package [**trueskill**](http://trueskill.org/).\n",
    "\n",
    "Since we have no information about players we will consider each team as a team from one player, same player in each match of that team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import trueskill\n",
    "from trueskill import BETA\n",
    "from trueskill.backends import cdf\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main process of ratin evaluation for TrueSkill is the same as for Elo rating. We just need to implement new function for update rating based on one match. And we also need to have ability to calculate chances to win. The estimation of win probabilities is based on intersect two normal distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using TrueSkill we should initialise it with parameters that fit to our game better. You may try to tune this parameters for better results, but now we use default values. The only thing that we change is the draw_probability, since the draw is impossible we set in to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trueskill.TrueSkill(mu=25.000, sigma=8.333, beta=4.167, tau=0.083, draw_probability=0.0%)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trueskill.setup(draw_probability=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TrueSkill_win_probability(player_rating, opponent_rating):\n",
    "    delta_mu = player_rating.mu - opponent_rating.mu\n",
    "    denom = sqrt(2 * (BETA * BETA) + pow(player_rating.sigma, 2) + pow(opponent_rating.sigma, 2))\n",
    "    return cdf(delta_mu / denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TrueSkill_update_rates(winer_rating, loser_rating):\n",
    "    return trueskill.rate_1vs1(winer_rating, loser_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can reimpliment Elo case main loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TS_ratings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index, row in matches_info.iterrows():\n",
    "    team_radiant = row['radiant']\n",
    "    team_dire = row['dire']\n",
    "    if team_radiant not in TS_ratings:\n",
    "        TS_ratings[team_radiant] = trueskill.Rating()\n",
    "    if team_dire not in TS_ratings:\n",
    "        TS_ratings[team_dire] = trueskill.Rating()\n",
    "    if  row['winner'] == \"RADIANT\":\n",
    "        TS_ratings[team_radiant], TS_ratings[team_dire] = TrueSkill_update_rates(TS_ratings[team_radiant], TS_ratings[team_dire])\n",
    "    else:\n",
    "        TS_ratings[team_dire], TS_ratings[team_radiant] = TrueSkill_update_rates(TS_ratings[team_dire], TS_ratings[team_radiant])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "team_sorted_by_TS = sorted(TS_ratings, key=lambda x:(TS_ratings[x].mu), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lv', 'iCCup', 'Super Strong', 'coL', 'EED']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team_sorted_by_TS[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[trueskill.Rating(mu=36.183, sigma=4.976),\n",
       " trueskill.Rating(mu=35.056, sigma=5.101),\n",
       " trueskill.Rating(mu=34.588, sigma=5.159),\n",
       " trueskill.Rating(mu=34.479, sigma=3.812),\n",
       " trueskill.Rating(mu=33.755, sigma=5.381)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[TS_ratings[team] for team in team_sorted_by_TS[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 5 team above is have nothing comon with Elos top 5. What does it mean?\n",
    "\n",
    "One of the possible answers is the lack of data. The standard deviation of all this teams is very large, so it means that real team perfomance may be almost any. We know that for normaly distributed variable the 95% confidence interval can be evaluated as $[\\mu-1.96*\\sigma,\\mu+1.96*\\sigma]$, so we can use the lower bound as a conservative estimate. We almost sure that rating is al least $\\mu-1.96*\\sigma$. Let sort the teams according conservative estimates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "team_sorted_by_TS_conservative = sorted(TS_ratings, key=lambda x:(TS_ratings[x].mu - 1.96*TS_ratings[x].sigma), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OG', 'EHOME', 'EG', 'VG', 'VP 2']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team_sorted_by_TS_conservative[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[trueskill.Rating(mu=33.187, sigma=0.809),\n",
       " trueskill.Rating(mu=33.006, sigma=0.801),\n",
       " trueskill.Rating(mu=32.591, sigma=0.798),\n",
       " trueskill.Rating(mu=32.472, sigma=0.798),\n",
       " trueskill.Rating(mu=32.329, sigma=0.799)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[TS_ratings[team] for team in team_sorted_by_TS_conservative[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we get the similar teams in slightly different ordes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already mentioned, TrueSkill also allow us to take the composition of teams into account. Lets try it.\n",
    "We will use another Dota 2 dataset. Use file prise_matches.csv. It slightly differs from previous one, but it provides us with players information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matches_info_players = pd.read_csv('prize_matches.csv').sort_values(['start_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "players_rates = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TrueSkill_win_probability_by_players(T1_ratings, T2_ratings):#You shoud use the list of players ratings\n",
    "    Team1_mu = 0\n",
    "    Team1_sigma = 0\n",
    "    Team2_mu = 0\n",
    "    Team2_sigma = 0\n",
    "    \n",
    "    for r in T1_ratings:\n",
    "        Team1_mu += r.mu\n",
    "        Team1_sigma += BETA * BETA + r.sigma * r.sigma\n",
    "    \n",
    "    for r in T2_ratings:\n",
    "        Team2_mu += r.mu\n",
    "        Team2_sigma += BETA * BETA + r.sigma * r.sigma\n",
    "\n",
    "    delta_mu = Team1_mu - Team2_mu\n",
    "    denom = sqrt(Team1_sigma + Team2_sigma)\n",
    "    return cdf(delta_mu / denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, row in matches_info_players.iterrows():\n",
    "    radiant_team_id = row['radiant_team_id']\n",
    "    dire_team_id = row['dire_team_id']\n",
    "    \n",
    "    radiant_team_players_ids = [] \n",
    "    dire_team_players_ids = [] \n",
    "    \n",
    "    for i in range(5):\n",
    "        radiant_team_players_ids.append(row['r'+str(i+1)+'_account_id'])\n",
    "        dire_team_players_ids.append(row['d'+str(i+1)+'_account_id'])\n",
    "    \n",
    "    radiant_team_players_ratings = []    \n",
    "\n",
    "    for p_id in radiant_team_players_ids:\n",
    "        if p_id not in players_rates:\n",
    "            players_rates[p_id] = trueskill.Rating()\n",
    "        radiant_team_players_ratings.append(players_rates[p_id])\n",
    "    \n",
    "    dire_team_players_ratings = []    \n",
    "\n",
    "    for p_id in dire_team_players_ids:\n",
    "        if p_id not in players_rates:\n",
    "            players_rates[p_id] = trueskill.Rating()\n",
    "        dire_team_players_ratings.append(players_rates[p_id])\n",
    "    \n",
    "    \n",
    "    if row['radiant_win']:\n",
    "        res = trueskill.rate([radiant_team_players_ratings, dire_team_players_ratings], ranks=[0, 1])\n",
    "    else:\n",
    "        res = trueskill.rate([radiant_team_players_ratings, dire_team_players_ratings], ranks=[1, 0])\n",
    "\n",
    "    for i in range(5):\n",
    "        players_rates[radiant_team_players_ids[i]] = res[0][i]\n",
    "        players_rates[dire_team_players_ids[i]] = res[1][i]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can find the top players. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "players_sorted_by_TS = sorted(players_rates, key=lambda x:(players_rates[x].mu), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3832921, 85844766, 31778508, 81904922, 110880087]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players_sorted_by_TS[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[trueskill.Rating(mu=42.075, sigma=4.853),\n",
       " trueskill.Rating(mu=41.728, sigma=4.684),\n",
       " trueskill.Rating(mu=41.728, sigma=4.684),\n",
       " trueskill.Rating(mu=41.613, sigma=4.704),\n",
       " trueskill.Rating(mu=37.602, sigma=4.100)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[players_rates[pl] for pl in players_sorted_by_TS[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "players_sorted_by_TS_conservative = sorted(players_rates, key=lambda x:(players_rates[x].mu - 1.96 *players_rates[x].sigma), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[111620041, 139876032, 86727555, 19672354, 38628747]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players_sorted_by_TS_conservative[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[trueskill.Rating(mu=36.350, sigma=1.294),\n",
       " trueskill.Rating(mu=36.000, sigma=1.308),\n",
       " trueskill.Rating(mu=35.555, sigma=1.240),\n",
       " trueskill.Rating(mu=35.248, sigma=1.249),\n",
       " trueskill.Rating(mu=35.423, sigma=1.355)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[players_rates[pl] for pl in players_sorted_by_TS_conservative[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you list teams of each of this top players?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are couple of task that can help you to check your understanding of code above.\n",
    "\n",
    "**Task 1:** In every rating evaluation we store only current rating for each team or player. Improve the code to store rating history. Can you draw a graph that shows the rating history for some team e.g. Virtus pro, or any other one?\n",
    "\n",
    "**Task 2:** We have not discussed assessing rating quality yet. Using predicted win probabilities, check which team has more chances to win before you update the ratings. \n",
    "Compare it to real results. Calculate accuracy (a share of correct predictions) for every 50 or 100 consecutive matches. Does accuracy tend to rise or to decrease with time?\n",
    "\n",
    "**Task 3:** If the team does not play for a long time we shoud increase the uncertanty estimate of its rating. You have a date of each match. Before update rating of teams, calculate the number of days from the previous match, and add this number muliplied by some coefficient (e.g. 0.01) to sigma value of this team rating. Does such penalty affect model quality? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
